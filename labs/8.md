# Application Portability
On occasion it may required to move an application off of a cluster or ensure that no traffic is routed to the cluster. To do this we will modify the deployment values within the deployment overlays that were used when we created the pacman application in the last lab.

Before we begin we will validate that every cluster is running the pacman application.
~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     0            0           6m21s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           6m21s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     0            0           6m21s
~~~

We would first like to ensure that cluster1 does not have any replicas running at all. To do this we will modify the file `~/federation-dev/labs/lab-7-assets/overlays/cluster1/07-pacman-deployment-rs.yaml`.

~~~sh
cd ~/federation-dev/labs/lab-7-assets/overlays/cluster1
sed -i 's/replicas: 1/replicas: 0/g' 07-pacman-deployment-rs.yaml
git commit -am 'removal of cluster1'
git push origin master
~~~

We will run the sync command to speed up the process of setting the replicas to 0.
~~~sh
argocd app sync cluster1-pacman
wait-for-argo-app cluster1-pacman
~~~


> **NOTE:** You can now go ahead and play Pacman, you should see that Pacman application requests are load balanced across all two remaining clusters.

We can also remove cluster3 which would only allow cluster2 to run the game.
~~~sh
cd ~/federation-dev/labs/lab-7-assets/overlays/cluster2
sed -i 's/replicas: 1/replicas: 0/g' 07-pacman-deployment-rs.yaml
git commit -am 'removal of cluster2'
git push origin master
~~~

Again, we will run the sync command to speed up the process of setting the replicas to 0 in cluster2.
~~~sh
argocd app sync cluster2-pacman
wait-for-argo-app cluster2-pacman
~~~

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   0/0     0            0           7m51s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   0/0     0            0           7m32s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           7m31s
~~~

> **NOTE:** You can now go ahead and play Pacman, you should see that Pacman application requests are sent to the one remaining cluster.

To populate the application back to all clusters, modify the replica count to be 1 for both cluster1 and cluster2.
~~~sh
cd ~/federation-dev/labs/lab-7-assets/overlays/cluster1
sed -i 's/replicas: 0/replicas: 1/g' 07-pacman-deployment-rs.yaml
cd ~/federation-dev/labs/lab-7-assets/overlays/cluster2
sed -i 's/replicas: 0/replicas: 1/g' 07-pacman-deployment-rs.yaml
git commit -am 'bring back cluster1 and cluster2'
git push origin master
~~~

We will run the sync command which will place pacman pods in all three clusters.

~~~sh
argocd app sync cluster1-pacman
wait-for-argo-app cluster1-pacman
argocd app sync cluster2-pacman
wait-for-argo-app cluster2-pacman
~~~

> NOTE: When running the sync command the resources may already be created because we set `--sync-policy automated` when creating the app in Argo CD.


Verify the deployments have the required replicas again in all three clusters.

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           24s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           9m24s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           25s
~~~

The most important thing to note during the modification of which clusters are running the
*pacman* application is that the scores persist regardless of which cluster the application is running and HAProxy always ensures the application is available.

Next Lab: [Lab 9 - Canary Deployments](./9.md)<br>
Previous Lab: [Lab 7 - Deploying Pacman](./7.md)<br>
[Home](./README.md)
