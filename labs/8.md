# Application Portability
On occasion it may required to move an application off of a cluster or ensure that no traffic is routed to the cluster. To do this we will modify the deployment values within the deployment overlays that were used when we created the pacman application in the last lab.

Before we begin we will validate that every cluster is running the pacman application.
~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     0            0           6m21s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           6m21s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     0            0           6m21s
~~~

We would first like to ensure that cluster1 does not have any replicas running at all. To do this we will modify the overlay file for the pacman deployment.

~~~sh
cd ~/federation-dev/labs/lab-7-assets
sed -i 's/replicas: 1/replicas: 0/g' overlays/cluster1/pacman-deployment.yaml
git commit -am 'Cluster1 replicas scaled to 0'
git push origin master
~~~

As the automatic sync is enabled in our application, the Argo CD controller will update the deployment settings. Let's check the application status:

~~~sh
argocd app sync cluster1-pacman
wait-for-argo-app cluster1-pacman
argocd app get cluster1-pacman
~~~

Now we should see no replicas running on cluster1:

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done
~~~

> **NOTE:** You can now go ahead and play Pacman, you should see that Pacman application requests are load balanced across all two remaining clusters.

We can also remove cluster3 which would only allow cluster2 to run the game.

~~~sh
sed -i 's/replicas: 1/replicas: 0/g' overlays/cluster2/pacman-deployment.yaml
git commit -am 'Cluster2 replicas scaled to 0'
git push origin master
~~~

Again, the automatic sync will take care of updating the deployment.

~~~sh
argocd app sync cluster2-pacman
wait-for-argo-app cluster2-pacman
argocd app get cluster2-pacman
~~~

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   0/0     0            0           7m51s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   0/0     0            0           7m32s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           7m31s
~~~

> **NOTE:** You can now go ahead and play Pacman, you should see that Pacman application requests are sent to the one remaining cluster.

To populate the application back to all clusters, modify the replica count to be 1 for both cluster1 and cluster2.

~~~sh
sed -i 's/replicas: 0/replicas: 1/g' overlays/cluster1/pacman-deployment.yaml
sed -i 's/replicas: 0/replicas: 1/g' overlays/cluster2/pacman-deployment.yaml
git commit -am 'Scale back cluster1 and cluster2 to 1'
git push origin master
~~~

In a few seconds the deployments will be synched with pacman pods in all three clusters.

~~~sh
argocd app sync cluster1-pacman
wait-for-argo-app cluster1-pacman
argocd app get cluster1-pacman
argocd app sync cluster2-pacman
wait-for-argo-app cluster2-pacman
argocd app get cluster2-pacman
~~~

Verify the deployments have the required replicas again in all three clusters.

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           24s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           9m24s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           25s
~~~

The most important thing to note during the modification of which clusters are running the
*pacman* application is that the scores persist regardless of which cluster the application is running and HAProxy always ensures the application is available.

Next Lab: [Lab 9 - Canary Deployments](./9.md)<br>
Previous Lab: [Lab 7 - Deploying Pacman](./7.md)<br>
[Home](./README.md)
