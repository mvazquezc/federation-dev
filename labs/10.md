# Disaster Recovery

As we have seen, the strength of GitOps is the ability to mange application workloads and move applications between clusters.

Through the use of overlays, GitoOps can be used to provide `Disaster Recovery` solutions for our applications.

In this lab we are going to see how GitOps along with our MongoDB cluster manages a failure of the primary MongoDB Replica.

## Creating some chaos
We are going to delete the deployment and the persistent volume claim within cluster1.

~~~sh
oc --context cluster1 -n mongo delete deployment mongo
oc --context cluster1 -n mongo delete pvc mongo
~~~

The above command states removes the persistent volume claim and the deployment of the MongoDB replica. To verify
no pods are running the following command can be ran.

> **NOTE:** It is possible that you see the pod in terminating status, that's fine.

When running this commmand. The output should show no resources are defined.

~~~sh
oc --context=cluster1 -n mongo get deployment
~~~

## Verifying Pacman Application still works

We have lost our primary MongoDB replica, but that didn't impact our application at all. Since we have three MongoDB replicas, the Pacman application can continue saving and reading high scores from the database.

You can go ahead and play Pacman, verify that high scores are saved.

## Bring the MongoDB replica back
We will now sync the app which will recreate the storage and redeploy the Mongo Replica
~~~sh
argocd app sync cluster1-mongo
wait-for-argo-app cluster1-mongo
~~~

We should see our MongoDB pod being created:

~~~sh 
oc --context=cluster1 -n mongo get pods

NAME                     READY   STATUS              RESTARTS   AGE
mongo-5f9ff55b67-5bgtc   0/1     ContainerCreating   0          8s
~~~

Once the pod is running the MongoDB replica will be reconfigured, we can get the new primary and secondary members by running:

~~~sh
wait-for-deployment cluster1 mongo mongo
wait-for-mongo-replicaset cluster1 mongo 3
~~~

# Using Gitops to Manage GitOps

Another way to handle disaster recovery is to store the Argo CD applications within the git repository. This would allow for us to recreate assets just by applying a YAML file defining the application.

Argo CD applications can be described and exported using the `oc` binary. This output could be exported to a YAML file and stored in the git repository.

~~~sh
oc get application -n argocd -o yaml cluster2-pacman --export
~~~

We will now delete the cluster2 pacman application and all of the YAML objects and then recreate them by applying a YAML file.

~~~sh
argocd app delete cluster2-pacman
~~~

Verify the items have been removed:

~~~sh
for i in svc deployment service sa; do oc get $i pacman -n pacman --context cluster2; done

Error from server (NotFound): services "pacman" not found
Error from server (NotFound): deployments.extensions "pacman" not found
Error from server (NotFound): services "pacman" not found
Error from server (NotFound): serviceaccounts "pacman" not found
~~~

We will now modify the file `labs/lab-10-assets/pacman-application.yaml` to reflect the git repository and the cluster2 url.

~~~sh
cd ~/federation-dev/labs/lab-10-assets
REPO=http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git
CLUSTER=$(argocd cluster list | grep cluster2 | awk '{print $1}')
sed -i "s~repoURL: repo~repoURL: $REPO~g" pacman-application.yaml
sed -i "s~server: server~server: $CLUSTER~g" pacman-application.yaml
git commit -am 'argocd managing cluster2 pacman'
git push origin master
~~~~

We now can use Argo CD to deploy the application within Argo CD ensuring that it is always there. By storing the Argo CD applications within git it allows us to manage the Argo CD applications in the same we manage deployments, services, and other Kubernetes objects.

~~~sh
argocd app create --project default --name argocd-applications --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-10-assets --dest-server $(argocd cluster list | grep cluster1 | awk '{print $1}') --dest-namespace argocd --revision master --sync-policy automated
~~~

When we run the sync command the application will be created within Argo CD and then the resources for cluster2 pacman will also be created similiar to when we run the `argocd app create` command.
~~~sh
argocd app sync argocd-applications
argocd app get cluster2-pacman
~~~

With this strategy all Argo CD applications that we created can be defined in YAML format and stored in git rather than using the `argocd` binary to create and define every Argo CD application.

This concludes the disaster recovery lab.

Next Lab: [Lab 11 - Wrapup](./11.md)<br>
Previous Lab: [Lab 9 - Canary Deployments](./9.md)<br>
[Home](./README.md)
